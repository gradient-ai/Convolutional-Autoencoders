{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  article dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as Datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as tqdm_regular\n",
    "import seaborn as sns\n",
    "from torchvision.utils import make_grid\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  configuring device\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  loading training data\n",
    "training_set = Datasets.CIFAR10(root='./', download=True,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "#  loading validation data\n",
    "validation_set = Datasets.CIFAR10(root='./', download=True, train=False,\n",
    "                                transform=transforms.ToTensor())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_each_class(dataset):\n",
    "  \"\"\"\n",
    "  This function searches for and returns\n",
    "  one image per class\n",
    "  \"\"\"\n",
    "  images = []\n",
    "  ITERATE = True\n",
    "  i = 0\n",
    "  j = 0\n",
    "\n",
    "  while ITERATE:\n",
    "    for label in tqdm_regular(dataset.targets):\n",
    "      if label==j:\n",
    "        images.append(dataset.data[i])\n",
    "        print(f'class {j} found')\n",
    "        i+=1\n",
    "        j+=1\n",
    "        if j==10:\n",
    "          ITERATE = False\n",
    "      else:\n",
    "        i+=1\n",
    "\n",
    "  return images\n",
    "  \n",
    "  \n",
    "#  extracting training images\n",
    "training_images = [x for x in training_set.data]\n",
    "\n",
    "#  extracting validation images\n",
    "validation_images = [x for x in validation_set.data]\n",
    "\n",
    "#  extracting test images for visualization purposes\n",
    "test_images = extract_each_class(validation_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  defining dataset class\n",
    "class CustomCIFAR10(Dataset):\n",
    "  def __init__(self, data, transforms=None):\n",
    "    self.data = data\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image = self.data[idx]\n",
    "\n",
    "    if self.transforms!=None:\n",
    "      image = self.transforms(image)\n",
    "    return image\n",
    "    \n",
    "    \n",
    "#  creating pytorch datasets\n",
    "training_data = CustomCIFAR10(training_images, transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "validation_data = CustomCIFAR10(validation_images, transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "test_data = CustomCIFAR10(test_images, transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autoencoder Architecture"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  defining encoder\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, in_channels=3, out_channels=16, latent_dim=200, act_fn=nn.ReLU()):\n",
    "    super().__init__()\n",
    "\n",
    "    self.net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1), # (32, 32)\n",
    "        act_fn,\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1), \n",
    "        act_fn,\n",
    "        nn.Conv2d(out_channels, 2*out_channels, 3, padding=1, stride=2), # (16, 16)\n",
    "        act_fn,\n",
    "        nn.Conv2d(2*out_channels, 2*out_channels, 3, padding=1),\n",
    "        act_fn,\n",
    "        nn.Conv2d(2*out_channels, 4*out_channels, 3, padding=1, stride=2), # (8, 8)\n",
    "        act_fn,\n",
    "        nn.Conv2d(4*out_channels, 4*out_channels, 3, padding=1),\n",
    "        act_fn,\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(4*out_channels*8*8, latent_dim),\n",
    "        act_fn\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(-1, 3, 32, 32)\n",
    "    output = self.net(x)\n",
    "    return output\n",
    "\n",
    "\n",
    "#  defining decoder\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, in_channels=3, out_channels=16, latent_dim=200, act_fn=nn.ReLU()):\n",
    "    super().__init__()\n",
    "\n",
    "    self.out_channels = out_channels\n",
    "\n",
    "    self.linear = nn.Sequential(\n",
    "        nn.Linear(latent_dim, 4*out_channels*8*8),\n",
    "        act_fn\n",
    "    )\n",
    "\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.ConvTranspose2d(4*out_channels, 4*out_channels, 3, padding=1), # (8, 8)\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(4*out_channels, 2*out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1), # (16, 16)\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(2*out_channels, 2*out_channels, 3, padding=1),\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(2*out_channels, out_channels, 3, padding=1, \n",
    "                           stride=2, output_padding=1), # (32, 32)\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(out_channels, out_channels, 3, padding=1),\n",
    "        act_fn,\n",
    "        nn.ConvTranspose2d(out_channels, in_channels, 3, padding=1)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.linear(x)\n",
    "    output = output.view(-1, 4*self.out_channels, 8, 8)\n",
    "    output = self.conv(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "#  defining autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.encoder.to(device)\n",
    "\n",
    "    self.decoder = decoder\n",
    "    self.decoder.to(device)\n",
    "\n",
    "  def forward(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convolutional Autoencoder Architecture"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConvolutionalAutoencoder():\n",
    "  def __init__(self, autoencoder):\n",
    "    self.network = autoencoder\n",
    "    self.optimizer = torch.optim.Adam(self.network.parameters(), lr=1e-3)\n",
    "\n",
    "  def train(self, loss_function, epochs, batch_size, \n",
    "            training_set, validation_set, test_set):\n",
    "    \n",
    "    #  creating log\n",
    "    log_dict = {\n",
    "        'training_loss_per_batch': [],\n",
    "        'validation_loss_per_batch': [],\n",
    "        'visualizations': []\n",
    "    } \n",
    "\n",
    "    #  defining weight initialization function\n",
    "    def init_weights(module):\n",
    "      if isinstance(module, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "      elif isinstance(module, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "\n",
    "    #  initializing network weights\n",
    "    self.network.apply(init_weights)\n",
    "\n",
    "    #  creating dataloaders\n",
    "    train_loader = DataLoader(training_set, batch_size)\n",
    "    val_loader = DataLoader(validation_set, batch_size)\n",
    "    test_loader = DataLoader(test_set, 10)\n",
    "\n",
    "    #  setting convnet to training mode\n",
    "    self.network.train()\n",
    "    self.network.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      print(f'Epoch {epoch+1}/{epochs}')\n",
    "      train_losses = []\n",
    "\n",
    "      #------------\n",
    "      #  TRAINING\n",
    "      #------------\n",
    "      print('training...')\n",
    "      for images in tqdm(train_loader):\n",
    "        #  zeroing gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        #  sending images to device\n",
    "        images = images.to(device)\n",
    "        #  reconstructing images\n",
    "        output = self.network(images)\n",
    "        #  computing loss\n",
    "        loss = loss_function(output, images.view(-1, 3, 32, 32))\n",
    "        #  calculating gradients\n",
    "        loss.backward()\n",
    "        #  optimizing weights\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #--------------\n",
    "        # LOGGING\n",
    "        #--------------\n",
    "        log_dict['training_loss_per_batch'].append(loss.item())\n",
    "\n",
    "      #--------------\n",
    "      # VALIDATION\n",
    "      #--------------\n",
    "      print('validating...')\n",
    "      for val_images in tqdm(val_loader):\n",
    "        with torch.no_grad():\n",
    "          #  sending validation images to device\n",
    "          val_images = val_images.to(device)\n",
    "          #  reconstructing images\n",
    "          output = self.network(val_images)\n",
    "          #  computing validation loss\n",
    "          val_loss = loss_function(output, val_images.view(-1, 3, 32, 32))\n",
    "\n",
    "        #--------------\n",
    "        # LOGGING\n",
    "        #--------------\n",
    "        log_dict['validation_loss_per_batch'].append(val_loss.item())\n",
    "\n",
    "\n",
    "      #--------------\n",
    "      # VISUALISATION\n",
    "      #--------------\n",
    "      print(f'training_loss: {round(loss.item(), 4)} validation_loss: {round(val_loss.item(), 4)}')\n",
    "\n",
    "      for test_images in test_loader:\n",
    "        #  sending test images to device\n",
    "        test_images = test_images.to(device)\n",
    "        with torch.no_grad():\n",
    "          #  reconstructing test images\n",
    "          reconstructed_imgs = self.network(test_images)\n",
    "        #  sending reconstructed and images to cpu to allow for visualization\n",
    "        reconstructed_imgs = reconstructed_imgs.cpu()\n",
    "        test_images = test_images.cpu()\n",
    "\n",
    "        #  visualisation\n",
    "        imgs = torch.stack([test_images.view(-1, 3, 32, 32), reconstructed_imgs], \n",
    "                          dim=1).flatten(0,1)\n",
    "        grid = make_grid(imgs, nrow=10, normalize=True, padding=1)\n",
    "        grid = grid.permute(1, 2, 0)\n",
    "        plt.figure(dpi=170)\n",
    "        plt.title('Original/Reconstructed')\n",
    "        plt.imshow(grid)\n",
    "        log_dict['visualizations'].append(grid)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "      \n",
    "    return log_dict\n",
    "\n",
    "  def autoencode(self, x):\n",
    "    return self.network(x)\n",
    "\n",
    "  def encode(self, x):\n",
    "    encoder = self.network.encoder\n",
    "    return encoder(x)\n",
    "  \n",
    "  def decode(self, x):\n",
    "    decoder = self.network.decoder\n",
    "    return decoder(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training \n",
    "## Bottleneck size: 200"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  training model\n",
    "model = ConvolutionalAutoencoder(Autoencoder(Encoder(), Decoder()))\n",
    "\n",
    "log_dict = model.train(nn.MSELoss(), epochs=10, batch_size=64, \n",
    "                       training_set=training_data, validation_set=validation_data,\n",
    "                       test_set=test_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bottleneck size: 1000"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  training model\n",
    "model = ConvolutionalAutoencoder(Autoencoder(Encoder(latent_dim=1000), Decoder(latent_dim=1000)))\n",
    "\n",
    "log_dict = model.train(nn.MSELoss(), epochs=10, batch_size=64, \n",
    "                       training_set=training_data, validation_set=validation_data,\n",
    "                       test_set=test_data)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}